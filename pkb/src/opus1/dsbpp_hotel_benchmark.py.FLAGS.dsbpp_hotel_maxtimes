import os
import posixpath
import logging
import time
import re
import glob
import json
import yaml
import requests

from absl import flags

from perfkitbenchmarker import configs
from perfkitbenchmarker import sample
from perfkitbenchmarker import vm_util
from perfkitbenchmarker import autoscale_util
from perfkitbenchmarker import linux_packages
from perfkitbenchmarker import data
from perfkitbenchmarker import errors
from perfkitbenchmarker import events, stages
from perfkitbenchmarker import os_types
from perfkitbenchmarker.traces import IsAnyTraceEnabled
from perfkitbenchmarker.traces import emon
from perfkitbenchmarker.linux_packages.dsb_common import time_to_ms

MEMCACHED_SERVICES = ["memcached-profile", "memcached-rate", "memcached-reserve"]
MONGODB_SERVICES = ["mongodb-geo", "mongodb-profile", "mongodb-rate", "mongodb-recommendation", "mongodb-reservation", "mongodb-user"]
APPLICATION_SERVICES = ["geo", "profile", "rate", "recommendation", "reservation", "search", "user"]
FRONTEND_SERVICES = ["frontend"]
TRACING_SERVICES = ["jaeger"]
TRACING_SERVICES_OUT = ["jaeger-out"]
MESH_SERVICES = ["consul"]
CLIENT_SERVICES = ["wrk2"]
REPLICABLE_SERVICES = FRONTEND_SERVICES + APPLICATION_SERVICES
CACHEDB_SERVICES = MEMCACHED_SERVICES + MONGODB_SERVICES

DEFAULT_CLUSTER_CLIENT_CONNECTION_COUNT = 32
DEFAULT_NAMESPACE = "hotel-res"

DSBPP_COMMIT_ID = "a932a6d5a9724cb1a43e0bd3f03061c4d967dab5"
DSBPP_URL = f"https://github.com/intel-sandbox/DeathStarBenchPlusPlus/archive/{DSBPP_COMMIT_ID}.tar.gz"
DSBPP_ARCHIVE = "DeathStarBenchPlusPlus.tgz"
DSBPP_DIRECTORY = "DeathStarBenchPlusPlus"

FLAGS = flags.FLAGS

# Cluster / Machine Configuration
flags.DEFINE_boolean("dsbpp_hotel_clients_in_cluster", False,
                     "When True, client nodes will be allocated in the K8s cluster.")
flags.DEFINE_integer("dsbpp_hotel_client_nodes", 1,
                     "Number of client nodes to provision. If dsbpp_hotel_clients_in_cluster is True, "
                     "these will be provisioned as part of the cluster, otherwise they will be VMs).",
                     lower_bound=1)
flags.DEFINE_integer("dsbpp_hotel_worker_nodes", 1,
                     "Number of worker nodes to provision. Relevant only if using PKB to provision the cluster.",
                     lower_bound=1)
flags.DEFINE_boolean("dsbpp_hotel_assign_nodes", False,
                     "When True, the frontend, bizlogic, and cache/database tiers will be assigned to specific "
                     "nodes. The following three flags must be set to specify the number of nodes assigned to "
                     "each tier.")
flags.DEFINE_integer("dsbpp_hotel_frontend_nodes", 1,
                     "Number of worker nodes allocated to front end services. "
                     "Only relevant when dsbpp_hotel_assign_nodes is True.",
                     lower_bound=1)
flags.DEFINE_integer("dsbpp_hotel_application_nodes", 1,
                     "Number of worker nodes allocated to application services. "
                     "Only relevant when dsbpp_hotel_assign_nodes is True.",
                     lower_bound=1)
flags.DEFINE_integer("dsbpp_hotel_cachedb_nodes", 1,
                     "Number of worker nodes allocated to cache and database services. "
                     "Only relevant when dsbpp_hotel_assign_nodes is True.",
                     lower_bound=1)
flags.DEFINE_integer("dsbpp_hotel_namespace_count", 1,
                     "Define the hotel-res system number to be deployed "
                     "concurrently. Each system will be settled down or "
                     "isolated in a specific namespace.")
flags.DEFINE_string("dsbpp_hotel_controlplane_path", "",
                     "When not empty string, holds the path to controlplane manifest")
flags.DEFINE_boolean("dsbpp_hotel_enable_crirm", False,
                     "Enable CRI-RM pod affinity and anti-affinity policy "
                     "for every container.")

# Service Replica Configuration
for service in REPLICABLE_SERVICES + CLIENT_SERVICES:
  flags.DEFINE_integer("dsbpp_hotel_" + service + "_replicas", 1, "The number of replicas for {}.".format(service), lower_bound=1)
flags.DEFINE_integer("dsbpp_hotel_replicas_override", 0,
                     "If greater than 0, this value will be used to set the number of replicas "
                     "for all replicable services.",
                     lower_bound=0)

# Client Run Options
flags.DEFINE_integer("dsbpp_hotel_client_instances_per_vm", 1,
                     "Number of client process instances that will be run on each client "
                     "machine. This only applies when dsbpp_hotel_clients_in_cluster is False.",
                     lower_bound=1)
flags.DEFINE_integer("dsbpp_hotel_client_connections", None,
                     "Number of connections from each client to the application. None "
                     "to set to num vCPUs on client machine (when dsbpp_hotel_clients_in_cluster is False), "
                     "otherwise set to default value: {}".format(DEFAULT_CLUSTER_CLIENT_CONNECTION_COUNT),
                     lower_bound=1)
flags.DEFINE_integer("dsbpp_hotel_client_threads", None,
                     "Number of client threads. None to set to minimum of vCPUs on "
                     "client and number of client connections (can't have more threads than "
                     "connections).",
                     lower_bound=1)
flags.DEFINE_integer("dsbpp_hotel_client_timeout", 5,
                     "Request timeout in seconds.",
                     lower_bound=1)
flags.DEFINE_integer("dsbpp_hotel_client_duration", 60,
                     "Number of seconds client(s) will make requests.",
                     lower_bound=1)
flags.DEFINE_boolean("dsbpp_hotel_client_connect_master_node", False,
                     "Only valid if dsbpp_hotel_clients_in_cluster is False. Use k8s master node for the wrk client to connect instead of worker node")


# Client Rate and Rate Auto-Scaling Configuration
flags.DEFINE_integer("dsbpp_hotel_client_rate", None,
                     "Client requests per second. Setting this will disable client "
                     "rate auto-scaling and run the given rate one time.",
                     lower_bound=1)
flags.DEFINE_integer("dsbpp_hotel_client_rate_autoscale_min", 500,
                     "Client requests per second for first iteration when auto-scaling rate.",
                     lower_bound=1)
flags.DEFINE_integer("dsbpp_hotelpp_client_rate_autoscale_increment", 100,
                     "Client requests per second to increment between rate auto-scaling iterations.",
                     lower_bound=1)
flags.DEFINE_integer("dsbpp_hotel_client_rate_autoscale_max", None,
                     "Client requests per second to short-circuit (exit) rate auto-scaling. "
                     "None allows auto-scaling to continue until throughput decreases.",
                     lower_bound=1)

# Workload Options
flags.DEFINE_string("dsbpp_hotel_namespace", DEFAULT_NAMESPACE,
                    "The namespace for single instance deployment or the"
                    "namespace prefix for multiple instances deployment that "
                    "will be used by the hotel microservices K8s resources.")
flags.DEFINE_string("dsbpp_hotel_dsb_archive", DSBPP_URL,
                    "The URL or local path to the DeathStarBench repo archive file.")
flags.DEFINE_string("dsbpp_hotel_github_token", None,
                   "Github OAUTH token to download DSBPlusPlus")
flags.DEFINE_string("dsbpp_hotel_image_cache", "amr-cache-registry.caas.intel.com/cache/",
                    "Set the image cache server and path that Kubernetes/Docker will use to "
                    "pull images. Set to empty string to pull from Docker Hub. If provided, "
                    "string must end with forward slash.")
flags.DEFINE_string("dsbpp_hotel_docker_image", "lianhao/dsbpp_hotel_reserv:rc2",
                    "The docker image for Hotel Reservation microservices")
flags.DEFINE_integer("dsbpp_hotel_hugepage_size", 0,
                     "The 2M hugepage size as well as the memory size resource limit for the application pod, in unit of MiB")
flags.DEFINE_string("dsbpp_hotel_node_label", None,
                    "A key/value that has been applied to cluster nodes to specify which nodes "
                    "can be used for this application, e.g., 'project=hotel-reservation")
flags.DEFINE_integer("dsbpp_hotel_p99_sla", 30,
                     "The P99 latency SLA in milliseconds. The maximum throughput "
                     "found with the P99 <= this value will be reported.")
flags.DEFINE_boolean("dsbpp_hotel_p99_sla_only", False,
                     "Stop the workload when SLA is exceeded.")
flags.DEFINE_float("dsbpp_hotel_error_limit", 0.05,
                   "The fraction of transactions that are allowed to be errors. "
                   "If this error rate is exceeded, the test will be terminated.")
ALL_WORKLOADS = ['mixed-workload_type_1']
flags.DEFINE_list("dsbpp_hotel_workloads",
                  ['mixed-workload_type_1'],
                  "The workloads to run. One or more of {}.".format(', '.join(ALL_WORKLOADS)))
flags.register_validator(
    'dsbpp_hotel_workloads',
    lambda workloads: workloads and set(workloads).issubset(ALL_WORKLOADS))
flags.DEFINE_boolean("dsbpp_hotel_skip_teardown", False,
                     "When set to True, don't teardown the application between runs and/or iterations.")
flags.DEFINE_boolean("dsbpp_hotel_deploy_jaeger", False,
                     "When set to True, the workload will deploy Jaeger into the cluster.")
flags.DEFINE_integer("dsbpp_hotel_gcpercent", 100,
		     "golang SetGCPercent value.")
flags.DEFINE_integer("dsbpp_hotel_memctimeout", 2,
		     "memcached client timeout value in seconds.")
flags.DEFINE_integer("dsbpp_hotel_maxtimes", 4,
                     "memcached client timeout value in seconds.")


BENCHMARK_NAME = "dsbpp_hotel"
BENCHMARK_CONFIG = """
dsbpp_hotel:
  description: DeathStarBenchPlusPlus Hotel Reservation
  vm_groups:
    client:
      os_type: ubuntu2004
      vm_spec: *default_dual_core
    controller:
      os_type: ubuntu2004
      vm_spec: *default_dual_core
    worker:
      os_type: ubuntu2004
      vm_spec: *default_dual_core
  container_specs: {}
  container_registry: {}
  container_cluster:
    vm_spec: *default_dual_core
"""

SOCIAL_NETWORK_DIR = posixpath.join(linux_packages.INSTALL_DIR, DSBPP_DIRECTORY, "socialNetwork")
HOTEL_RES_DIR = posixpath.join(linux_packages.INSTALL_DIR, DSBPP_DIRECTORY, "hotelReservation")
WRK2_DIR = posixpath.join(SOCIAL_NETWORK_DIR, "wrk2")
WRK2_SCRIPTS_DIR = posixpath.join(HOTEL_RES_DIR, 'wrk2_lua_scripts')

WEB_FRONTEND_PORT = 5000
JAEGER_PORT = 16686

CONFIG_MAPS_APP = {
    "configmap-config-json": "configmaps/config.json"
}
AFFINITY_POLICY = "{container}:\n  - scope:\n      key: pod/namespace\n      operator: In\n      values:\n        - {namespace}\n    match:\n      operator: AlwaysTrue\n    weight: 111\n"

ANTI_AFFINITY_POLICY = "{container}:\n  - scope:\n      key: pod/namespace\n      operator: NotIn\n      values:\n        - {namespace}\n    match:\n      operator: AlwaysTrue\n    weight: 111\n"

AFFINITY_KEY = "cri-resource-manager.intel.com/affinity"

ANTI_AFFINITY_KEY = "cri-resource-manager.intel.com/anti-affinity"

from collections import OrderedDict

class literal(str):
    pass

class ShadowOrderedDict(OrderedDict):
    pass

def literal_presenter(dumper, data):
    return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='|')
yaml.representer.SafeRepresenter.add_representer(literal, literal_presenter)


def ordered_dict_presenter(dumper, data):
    return dumper.represent_dict(data.items())
yaml.representer.SafeRepresenter.add_representer(ShadowOrderedDict, ordered_dict_presenter)


def _GetNameSpaceName(inst_id=None):
    """Format dsb hotel namespace.
    @inst_id: the id number
    """
    if (FLAGS.dsbpp_hotel_namespace_count == 1 or inst_id is None):
        return FLAGS.dsbpp_hotel_namespace
    return "{}{}".format(FLAGS.dsbpp_hotel_namespace, inst_id)


def _IsManagedCluster():
  # if kubeconfig is in the temp dir, it is assumed to have been created by PKB
  # when provisioning a cloud cluster, e.g., EKS
  if FLAGS.kubeconfig:
    return FLAGS.kubeconfig.startswith(vm_util.GetTempDir())
  else:
    return True


def GetConfig(user_config):
  config = configs.LoadConfig(BENCHMARK_CONFIG, user_config, BENCHMARK_NAME)
  client_vm_count = FLAGS.dsbpp_hotel_client_nodes if not FLAGS.dsbpp_hotel_clients_in_cluster else 0
  config['vm_groups']['client']['vm_count'] = client_vm_count

  if FLAGS.emon:
    FLAGS.benchmark_control_emon_post_process = True
  FLAGS.trace_allow_benchmark_control = True

  if not FLAGS.kubeconfig:
    raise errors.Benchmarks.PrepareException("Invalid kubeconfig file. Please use the correct kubeconfig file on the controller node.")

  if FLAGS.dsbpp_hotel_controlplane_path and FLAGS.dsbpp_hotel_enable_crirm:
    raise errors.Benchmarks.PrepareException("Use both of controlplan and CRIRM are prohibitied.")

  if _IsManagedCluster():
    client_node_count = FLAGS.dsbpp_hotel_client_nodes if FLAGS.dsbpp_hotel_clients_in_cluster else 0
    worker_node_count = FLAGS.dsbpp_hotel_frontend_nodes + FLAGS.dsbpp_hotel_application_nodes + FLAGS.dsbpp_hotel_cachedb_nodes if FLAGS.dsbpp_hotel_assign_nodes else FLAGS.dsbpp_hotel_worker_nodes
    cluster_node_count = client_node_count + worker_node_count
    config['container_cluster']['vm_count'] = cluster_node_count
    del config['vm_groups']['controller']
    del config['vm_groups']['worker']
  else:  # will provision a cloud cluster
    del config['container_cluster']
    del config['container_specs']
    del config['container_registry']
  return config


def CheckPrerequisites(benchmark_config):
  pass


def _GetClients(benchmark_spec):
  return benchmark_spec.vm_groups["client"] if not FLAGS.dsbpp_hotel_clients_in_cluster else []


def _GetWorkers(benchark_spec):
  return benchark_spec.vm_groups["worker"]


def _GetControlNode(benchmark_spec):
  return benchmark_spec.vm_groups["controller"][0]


def _GetTotalClientConnectionCount(benchmark_spec):
  """ Get the total (all clients included) connection count """
  client_count = len(_GetClients(benchmark_spec)) * \
                 FLAGS.dsbpp_hotel_namespace_count * \
                 FLAGS.dsbpp_hotel_client_instances_per_vm or \
                 FLAGS.dsbpp_hotel_replicas_override or \
                 FLAGS.dsbpp_hotel_wrk2_replicas
  return _GetClientConnectionCount(benchmark_spec) * client_count


def _GetClientConnectionCount(benchmark_spec):
  """ Get the number of connections for a single client """
  if FLAGS.dsbpp_hotel_clients_in_cluster:
    return FLAGS.dsbpp_hotel_client_connections or DEFAULT_CLUSTER_CLIENT_CONNECTION_COUNT
  else:
    return FLAGS.dsbpp_hotel_client_connections or _GetClients(benchmark_spec)[0].NumCpusForBenchmark()


def _GetClientThreadCount(benchmark_spec):
  """ Get the number of threads for a single client """
  # threads must be less than or equal to connections
  if FLAGS.dsbpp_hotel_clients_in_cluster:
    return (FLAGS.dsbpp_hotel_client_threads or
            _GetClientConnectionCount(benchmark_spec))
  else:
    return (FLAGS.dsbpp_hotel_client_threads or
            min(_GetClientConnectionCount(benchmark_spec),
                _GetClients(benchmark_spec)[0].NumCpusForBenchmark()))


def _GetKubectlPrefix(namespace=None):
  cmd = [FLAGS.kubectl, '--kubeconfig={}'.format(FLAGS.kubeconfig)]
  if namespace:
    cmd.append('--namespace={}'.format(namespace))
  return cmd


def _GetKubectlCmd(cmd, namespace=None):
  return _GetKubectlPrefix(namespace) + cmd


def _RunKubectl(benchmark_spec,
                cmd,
                inst_id=None,
                append_namespace=True,
                suppress_failure=False,
                suppress_warning=False,
                force_info_log=False,
                timeout=vm_util.DEFAULT_TIMEOUT):

  namespace = _GetNameSpaceName(inst_id) if append_namespace else None
  kubectl_cmd = _GetKubectlCmd(cmd, namespace)
  if _IsManagedCluster():
    return vm_util.IssueCommand(kubectl_cmd,
                                raise_on_failure=not suppress_failure,
                                suppress_warning=suppress_warning,
                                force_info_log=force_info_log,
                                timeout=timeout)
  else:
    return _GetControlNode(benchmark_spec).RemoteCommandWithReturnCode(' '.join(kubectl_cmd), ignore_failure=suppress_failure, suppress_warning=suppress_warning, should_log=force_info_log, timeout=timeout)


def _GetGetNodesCommand(format, master=False):
  if master:
    selectors = ["node-role.kubernetes.io/master"]
  else:
    selectors = ["!node-role.kubernetes.io/master"]
  if FLAGS.dsbpp_hotel_node_label is not None:
    selectors.append(FLAGS.dsbpp_hotel_node_label)
  return ['get', 'nodes', '--selector={}'.format(','.join(selectors)), '-o', format]


def _GetClusterWorkerNodes(benchmark_spec):
  cmd = _GetGetNodesCommand('jsonpath={.items..metadata.name}')
  return _RunKubectl(benchmark_spec, cmd, append_namespace=False)[0].split()


def _DeleteNamespace(benchmark_spec, inst_id):
  try:
      _RunKubectl(benchmark_spec,
                  ['delete', 'namespace', _GetNameSpaceName(inst_id)],
                  append_namespace=False, suppress_failure=True, timeout=600)
  except errors.VmUtil.IssueCommandTimeoutError:
      # force delete pod in case pod hung in 'terminating' status
      _RunKubectl(benchmark_spec,
                  ['delete', 'pod', '--all', '--force', '--grace-period=0'],
                  inst_id=inst_id,
                  append_namespace=True,
                  suppress_failure=True,
                  suppress_warning=True)
      _RunKubectl(benchmark_spec,
                  ['delete', 'namespace', _GetNameSpaceName(inst_id)],
                  inst_id=inst_id,
                  append_namespace=False,
                  suppress_failure=True,
                  timeout=600)


def _CreateNamespace(benchmark_spec, inst_id):
  _RunKubectl(benchmark_spec, ['create', 'namespace', _GetNameSpaceName(inst_id)], append_namespace=False)


def _GetNodePort(benchmark_spec, service, inst_id):
  num_waits_left = FLAGS.k8s_get_retry_count
  web_port = ""
  while not web_port and num_waits_left:
    web_port, _, _ = _RunKubectl(benchmark_spec,
                                 ['get', '-o', 'jsonpath="{.spec.ports[0].nodePort}"', 'services', service],
                                 inst_id=inst_id,
                                 suppress_failure=True,
                                 suppress_warning=True)
    if not web_port:
      time.sleep(FLAGS.k8s_get_wait_interval)
      num_waits_left -= 1
  if not web_port:
    raise errors.Benchmarks.PrepareException(service + " port not found.")
  return web_port.strip("\"")


def _GetWebPort(benchmark_spec, service, inst_id):
  if _IsManagedCluster():
    if service == "frontend":
      return WEB_FRONTEND_PORT
    elif service == "jaeger-out":
      return JAEGER_PORT
  else:
    return _GetNodePort(benchmark_spec, service, inst_id)
  raise errors.Benchmarks.PrepareException("Unknown service.")


def _GetWebIpAddress(benchmark_spec, service, inst_id):
  web_ip = ""
  num_waits_left = FLAGS.k8s_get_retry_count
  while not web_ip and num_waits_left:
    if _IsManagedCluster():
      stdout, stderr, _ = _RunKubectl(benchmark_spec,
                                      ['get', 'svc', service, '-ojsonpath={.status.loadBalancer.ingress[*].hostname}'],
                                      inst_id=inst_id,
                                      suppress_failure=True,
                                      suppress_warning=True)
    else:
      stdout, stderr, _ = _RunKubectl(benchmark_spec,
                                      _GetGetNodesCommand('json', master=FLAGS.dsbpp_hotel_client_connect_master_node),
                                      inst_id=inst_id,
                                      suppress_failure=True,
                                      suppress_warning=True)
    if stderr:
      raise errors.Benchmarks.PrepareException("Error received from kubectl get: " + stderr)
    if not stdout:
      time.sleep(FLAGS.k8s_get_wait_interval)
      num_waits_left -= 1
      continue
    if _IsManagedCluster():
      web_ip = stdout
    else:
      addresses = json.loads(stdout)["items"][0]["status"]["addresses"]
      for address in addresses:
        if address["type"] == "InternalIP":
          web_ip = address["address"]
  if not web_ip:
    raise errors.Benchmarks.PrepareException(service + ' IP address not found.')
  return web_ip


def _GetSvcIpAddress(benchmark_spec, service, inst_id):
  return _RunKubectl(benchmark_spec,
                     ['get', 'svc', service, '-ojsonpath={.spec.clusterIP}'],
                     inst_id=inst_id)[0]


def _GetWrk2Pods(benchmark_spec, inst_id):
  output = _RunKubectl(benchmark_spec,
                       ['get', 'pods', '-l', 'app=wrk2', '-o' 'custom-columns=:metadata.name'],
                       inst_id=inst_id)[0]
  return [line for line in output.splitlines() if line]


def _UpdateReplicas(benchmark_spec, inst_id):
  num_client_pods = len(_GetWrk2Pods(benchmark_spec, inst_id))
  for deployment in REPLICABLE_SERVICES:
    if num_client_pods == 0 and deployment == "wrk2":
      continue
    replicas = FLAGS.dsbpp_hotel_replicas_override or FLAGS.flag_values_dict()["dsbpp_hotel_" + deployment + "_replicas"]
    _RunKubectl(benchmark_spec,
                ['scale', '--replicas', str(replicas), "deployment/{}".format(deployment)],
                inst_id=inst_id)


def _WaitForWebServer(vm, web_frontend_ip, web_frontend_port):
  logging.info("Waiting for web server.")
  response = ""
  num_waits_left = FLAGS.k8s_get_retry_count
  while num_waits_left:
    response = vm.RemoteCommand("wget --no-proxy --spider -S \"{}:{}\" 2>&1".format(web_frontend_ip, web_frontend_port) + r" | awk '/HTTP\// {print $2}'")[0].strip()
    if response == "200":
      break
    logging.info("Sleeping for {} seconds.".format(FLAGS.k8s_get_wait_interval))
    time.sleep(FLAGS.k8s_get_wait_interval)
    num_waits_left -= 1
  if not response:
    raise errors.Benchmarks.PrepareException("Timeout waiting to connect to web server.")


def _WaitForPods(benchmark_spec, inst_id=None, wait_ctlplane=False):
  # make sure the pods have been scheduled
  logging.info("Waiting for all pods to be scheduled.")
  all_running = False
  num_waits_left = FLAGS.k8s_get_retry_count
  while num_waits_left and not all_running:
    all_running = True
    if wait_ctlplane:
      result = _RunKubectl(benchmark_spec,
                           ['get', 'pods', '-o', 'json', '-l', 'app=ctlplane-daemonset'],
                           append_namespace=False,
                           suppress_failure=True)[0]
    else:
      result = _RunKubectl(benchmark_spec,
                           ['get', 'pods', '-o', 'json'],
                           inst_id=inst_id)[0]
    pods = json.loads(result)
    for pod in pods["items"]:
      if 'conditions' not in pod["status"]:
        all_running = False
        break
      for condition in pod["status"]["conditions"]:
        if not condition["status"]:
          all_running = False
          break
      if not all_running:
        break
    logging.info("Sleeping for {} seconds.".format(FLAGS.k8s_get_wait_interval))
    time.sleep(FLAGS.k8s_get_wait_interval)
    num_waits_left -= 1
  if not all_running:
    raise errors.Benchmarks.PrepareException("Timeout waiting for all pods to be scheduled.")

  # make sure the containers are ready
  logging.info("Waiting for all containers to enter ready state.")
  all_running = False
  num_waits_left = FLAGS.k8s_get_retry_count
  while num_waits_left and not all_running:
    all_running = True
    if wait_ctlplane:
      result = _RunKubectl(benchmark_spec,
                           ['get', 'pods', '-o', 'json', '-l', 'app=ctlplane-daemonset'],
                           append_namespace=False,
                           suppress_failure=True)[0]
    else:
      result = _RunKubectl(benchmark_spec, ['get', 'pods', '-o', 'json'], inst_id=inst_id)[0]
    pods = json.loads(result)
    for pod in pods["items"]:
      if "containerStatuses" not in pod["status"]:
        all_running = False
        break
      for cstatus in pod["status"]["containerStatuses"]:
        if not cstatus["ready"]:
          all_running = False
          break
      if not all_running:
        break
    logging.info("Sleeping for {} seconds.".format(FLAGS.k8s_get_wait_interval))
    time.sleep(FLAGS.k8s_get_wait_interval)
    num_waits_left -= 1
  if not all_running:
    raise errors.Benchmarks.PrepareException("Timeout waiting for all containers to enter ready state.")


def _CreateConfigMaps(benchmark_spec, config_maps, inst_id):
  if _IsManagedCluster():
    deploydir = os.path.join(vm_util.GetTempDir(), 'deploy{}'.format(inst_id))
  else:
    deploydir = posixpath.join(linux_packages.INSTALL_DIR, 'deploy{}'.format(inst_id))

  for name, file in config_maps.items():
    file = posixpath.join(deploydir, file)
    _RunKubectl(benchmark_spec,
                ['create', 'configmap', name, '--from-file={}'.format(posixpath.join(file))],
                inst_id=inst_id,
                suppress_failure=True)


def _TweakConfigMaps(benchmark_spec, config_maps, inst_id):
  rootdir = os.path.join(vm_util.GetTempDir(), DSBPP_DIRECTORY, "hotelReservation", "k8s")
  if _IsManagedCluster():
    deploydir = os.path.join(vm_util.GetTempDir(), 'deploy{}'.format(inst_id))
  else:
    deploydir = posixpath.join(linux_packages.INSTALL_DIR, 'deploy{}'.format(inst_id))
  for _, filepath in config_maps.items():
    text = ""
    with open(os.path.join(rootdir, filepath), 'r') as f:
      text = f.read()
    text = text.replace(DEFAULT_NAMESPACE, _GetNameSpaceName(inst_id))
    if _IsManagedCluster():
      vm_util.IssueCommand(['mkdir', '-p', os.path.join(deploydir, os.path.split(filepath)[0])])
      with open(os.path.join(deploydir, filepath), 'w') as f:
        f.write(text)
    else:
      _GetControlNode(benchmark_spec).RemoteCommand('mkdir -p ' + os.path.join(deploydir, os.path.split(filepath)[0]))
      vm_util.CreateRemoteFile(_GetControlNode(benchmark_spec), text, posixpath.join(deploydir, filepath))


def _DeployPods(benchmark_spec, inst_id):
  """ In order to decrease the running time of pkb. First of all, we deploy consul services.
  Secondly, we deploy memcached services then we deploy mongo services. At last, we deploy
  the rest of business services.
  """

  deploydir = os.path.join(vm_util.GetTempDir(), 'dsbpp_deploy{}'.format(inst_id))
  remote_deploydir = posixpath.join(linux_packages.INSTALL_DIR,
                                    'deploy{}'.format(inst_id))
  if not _IsManagedCluster():
    _GetControlNode(benchmark_spec).RemoteCommand('rm -rf {0} && mkdir {0}'.format(remote_deploydir))

  p1 = "|".join(MESH_SERVICES)

  p2 = "|".join(MEMCACHED_SERVICES)

  p3 = "|".join(MONGODB_SERVICES)

  mesh_files = set([f for f in os.listdir(deploydir) if re.match(p1, f)])
  for yaml_file in mesh_files:
    yaml_file = os.path.join(deploydir, yaml_file)
    if not _IsManagedCluster():
      _GetControlNode(benchmark_spec).PushFile(yaml_file, remote_deploydir)
      yaml_file = posixpath.join(remote_deploydir, os.path.split(yaml_file)[1])
    _RunKubectl(benchmark_spec, ['apply', '-f', yaml_file], inst_id=inst_id)

  memcached_files = set([f for f in os.listdir(deploydir) if re.match(p2, f)])
  for yaml_file in memcached_files:
    yaml_file = os.path.join(deploydir, yaml_file)
    if not _IsManagedCluster():
      _GetControlNode(benchmark_spec).PushFile(yaml_file, remote_deploydir)
      yaml_file = posixpath.join(remote_deploydir, os.path.split(yaml_file)[1])
    _RunKubectl(benchmark_spec, ['apply', '-f', yaml_file], inst_id=inst_id)

  mongo_files = set([f for f in os.listdir(deploydir) if re.match(p3, f)])
  for yaml_file in mongo_files:
    yaml_file = os.path.join(deploydir, yaml_file)
    if not _IsManagedCluster():
      _GetControlNode(benchmark_spec).PushFile(yaml_file, remote_deploydir)
      yaml_file = posixpath.join(remote_deploydir, os.path.split(yaml_file)[1])
    _RunKubectl(benchmark_spec, ['apply', '-f', yaml_file], inst_id=inst_id)

  all_files = set([f for f in os.listdir(deploydir)])
  business_files = all_files - mongo_files - memcached_files - mesh_files
  for yaml_file in business_files:
    yaml_file = os.path.join(deploydir, yaml_file)
    if not _IsManagedCluster():
      _GetControlNode(benchmark_spec).PushFile(yaml_file, remote_deploydir)
      yaml_file = posixpath.join(remote_deploydir, os.path.split(yaml_file)[1])
    _RunKubectl(benchmark_spec, ['apply', '-f', yaml_file], inst_id=inst_id)


def _GetPods(benchmark_spec, inst_id):
  output = _RunKubectl(benchmark_spec,
                       ['get', 'pods', '-o', 'custom-columns=:metadata.name'],
                       inst_id=inst_id)[0]
  return [line for line in output.splitlines() if line]


def _DeployCtlPlane(benchmark_spec):
  remote_deploydir = posixpath.join(linux_packages.INSTALL_DIR, 'ctlplane_deploy')
  if not _IsManagedCluster():
    _GetControlNode(benchmark_spec).RemoteCommand('rm -rf {0} && mkdir {0}'.format(remote_deploydir))

  ctlplane_files = set([f for f in os.listdir(FLAGS.dsbpp_hotel_controlplane_path)])
  ctlplane_dirname = os.path.dirname(FLAGS.dsbpp_hotel_controlplane_path)
  for f in ctlplane_files:
    yaml_file = posixpath.join(ctlplane_dirname, f)
    _GetControlNode(benchmark_spec).PushFile(yaml_file, remote_deploydir)
    yaml_file = posixpath.join(remote_deploydir, os.path.split(yaml_file)[1])
    _RunKubectl(benchmark_spec,
                ['apply', '-f', yaml_file],
                append_namespace=False,
                suppress_failure=True,
                timeout=600)
  _WaitForPods(benchmark_spec, inst_id=None, wait_ctlplane=True)
 
def _DeployApplication(benchmark_spec):
  if (FLAGS.dsbpp_hotel_namespace_count == 0):
    raise errors.Benchmarks.PrepareException(
      "dsbpp_hotel_namespace_count could not be set to 0")
  #NOTE(changzhi) Push ctlplane files to controller and deploy
  if FLAGS.dsbpp_hotel_controlplane_path:
    _DeployCtlPlane(benchmark_spec)
    # NOTE(llu) control plane requires workload to be deployed in sequence
    for inst_id in range(FLAGS.dsbpp_hotel_namespace_count):
      vm_util.RunThreaded(_DeployApplicationInstance,
                          [((benchmark_spec, inst_id), {})])
  else:
    vm_util.RunThreaded(_DeployApplicationInstance,
                        [((benchmark_spec, inst_id), {})
                         for inst_id in range(FLAGS.dsbpp_hotel_namespace_count)])


def _TearDownControlplane(benchmark_spec):
  remote_deploydir = posixpath.join(linux_packages.INSTALL_DIR, 'ctlplane_deploy')
  ctlplane_files = set([f for f in os.listdir(FLAGS.dsbpp_hotel_controlplane_path)])
  ctlplane_dirname = os.path.dirname(FLAGS.dsbpp_hotel_controlplane_path)
  for f in ctlplane_files:
    yaml_file = posixpath.join(ctlplane_dirname, f)
    yaml_file = posixpath.join(remote_deploydir, os.path.split(yaml_file)[1])
    _RunKubectl(benchmark_spec,
                ['delete', '-f', yaml_file],
                append_namespace=False,
                suppress_failure=True,
                timeout=600)
 
def _DeployApplicationInstance(benchmark_spec, inst_id):
  previously_deployed = len(_GetPods(benchmark_spec, inst_id)) > 0
  if not previously_deployed:
    _CreateNamespace(benchmark_spec, inst_id)
    _TweakConfigMaps(benchmark_spec, CONFIG_MAPS_APP, inst_id)

    if _IsManagedCluster():
      root_dir = os.path.join(vm_util.GetTempDir(), 'dsbpp_deploy')
    else:
      root_dir = posixpath.join(linux_packages.INSTALL_DIR, 'dsbpp_deploy')

    _CreateConfigMaps(benchmark_spec, CONFIG_MAPS_APP, inst_id)
    _DeployPods(benchmark_spec, inst_id)
  _UpdateReplicas(benchmark_spec, inst_id)
  _WaitForPods(benchmark_spec, inst_id)
  if not FLAGS.dsbpp_hotel_clients_in_cluster:
    web_ip_host = _GetWebIpAddress(benchmark_spec, "frontend", inst_id)
    web_port = _GetWebPort(benchmark_spec, "frontend", inst_id)
    _WaitForWebServer(_GetClients(benchmark_spec)[0], web_ip_host, web_port)


def _TeardownApplication(benchmark_spec):
  vm_util.RunThreaded(_DeleteNamespace,
                      [((benchmark_spec, inst_id), {})
                       for inst_id in range(FLAGS.dsbpp_hotel_namespace_count)])
  if (FLAGS.dsbpp_hotel_controlplane_path != ""):
    _TearDownControlplane(benchmark_spec)
  # we don't need to delete persistent volume, since we tweak the manifests not using persistent volume
  #_RunKubectl(benchmark_spec, ['delete', 'pv', '-l', 'death-star-project==hotel-res'], append_namespace=False, suppress_failure=True)


def _SavePodConfig(benchmark_spec, label):
  filename = vm_util.PrependTempDir('pod_config.txt')
  with open(filename, 'a') as f:
    f.write(f"\n{label}:\n")
    pod_config = _RunKubectl(benchmark_spec,
                             ['get', 'pods', '-o', 'wide', '--all-namespaces'],
                             append_namespace=False,
                             force_info_log=True)[0]
    f.write(pod_config)


def Prepare(benchmark_spec):

  def _PrepareClients(clients):
    def _InstallDependencies(vm):
      vm.Install("build_tools")
      vm.Install("openssl")
      vm.InstallPackages("wget python3-pip")
      vm.RemoteCommand("sudo -E pip3 install asyncio && sudo -E pip3 install aiohttp")
      if vm.BASE_OS_TYPE == os_types.DEBIAN:
        vm.InstallPackages("luarocks zlib1g-dev apt-transport-https ca-certificates curl software-properties-common unzip")
        vm.RemoteCommand("sudo -E luarocks install luasocket 3.0rc1-2")
      elif vm.BASE_OS_TYPE == os_types.RHEL:
        vm.InstallPackages("zlib-devel ca-certificates curl unzip")
        stdout, stderr = vm.RemoteCommand("lua -v")
        if not stderr.startswith("Lua 5.1"):
          raise errors.Benchmarks.PrepareException("Please manually install Lua 5.1 https://www.lua.org/ftp/lua-5.1.4.tar.gz")
        try:
          _, _ = vm.RemoteCommand("which luarocks")
        except errors.VirtualMachine.RemoteCommandError:
          # No luarocks we need to install it
          luarocks_tmpdir = "/tmp/pkb_dsbpp_luarocks_install"
          vm.RemoteCommand(f"mkdir {luarocks_tmpdir}")
          vm.RemoteCommand(f"cd {luarocks_tmpdir} && wget http://luarocks.github.io/luarocks/releases/luarocks-3.8.0.tar.gz && tar zxpf luarocks-3.8.0.tar.gz")
          vm.RemoteCommand(f"cd {luarocks_tmpdir}/luarocks-3.8.0 && ./configure --with-lua-include=/usr/local/include && make && sudo make install")
          vm.RemoteCommand(f"rm -rf {luarocks_tmpdir}")
        vm.RemoteCommand("sudo -E luarocks install luasocket 3.0rc1-2")
      else:
        raise errors.Benchmarks.PrepareException(f"Unsupported OS type {vm.OS_TYPE}")

    def _InstallWrk2(client):
      local_dsb_archive = os.path.join(vm_util.GetTempDir(), DSBPP_DIRECTORY, DSBPP_ARCHIVE)
      remote_dsb_directory = posixpath.join(linux_packages.INSTALL_DIR, DSBPP_DIRECTORY)
      remote_dsb_archive = posixpath.join(remote_dsb_directory, DSBPP_ARCHIVE)
      client.RemoteCommand("mkdir -p {}".format(remote_dsb_directory))
      client.PushFile(local_dsb_archive, remote_dsb_directory)
      client.RemoteCommand("tar -xf {} --strip-components=1 -C {}".format(remote_dsb_archive, remote_dsb_directory))
      client.RemoteCommand("cd {} && sed -i ' 1 s/.*/& -fPIC/' Makefile && make clean && make".format(WRK2_DIR))

    def _SetMaxOpenFiles(vm, nofile):
      before = int(vm.RemoteCommand('ulimit -n')[0])
      logging.info("vm ulimit -n (before): {}".format(before))
      logging.info("vm nofile minimum needed: {}".format(nofile))
      if nofile > before:
        vm.RemoteCommand('echo "* soft nofile {}" | sudo tee -a /etc/security/limits.conf'.format(nofile))
        vm.RemoteCommand('echo "* hard nofile {}" | sudo tee -a /etc/security/limits.conf'.format(nofile))
        logging.info("vm ulimit -n (after): {}".format(vm.RemoteCommand('ulimit -n')[0]))

    vm_util.RunThreaded(_InstallDependencies, clients)
    vm_util.RunThreaded(_InstallWrk2, clients)
    arbitrary_fudge_factor = 4
    nofiles = _GetClientConnectionCount(benchmark_spec) * FLAGS.dsbpp_hotel_client_instances_per_vm * arbitrary_fudge_factor
    vm_util.RunThreaded(_SetMaxOpenFiles, [((client, nofiles), {}) for client in clients])

  def _PrepareCluster(benchmark_spec):

    def _ConfigureNodeSelectors(benchmark_spec):
      def _LabelClusterNode(benchmark_spec, node, key, value):
        _RunKubectl(benchmark_spec, ['label', '--overwrite', 'nodes', node, '{}={}'.format(key, value)], append_namespace=False)

      def _ClearNodeAssignments(benchmark_spec, nodes):
        for node in nodes:
          for selector in ["frontend", "application", "cachedb", "client"]:
            _RunKubectl(benchmark_spec, ['label', 'nodes', node, '{}-'.format(selector)], append_namespace=False, suppress_failure=True)

      nodes = _GetClusterWorkerNodes(benchmark_spec)
      _ClearNodeAssignments(benchmark_spec, nodes)
      if FLAGS.dsbpp_hotel_assign_nodes:
        total_node_count = len(nodes)
        client_node_count = FLAGS.dsbpp_hotel_client_nodes if FLAGS.dsbpp_hotel_clients_in_cluster else 0
        min_required_node_count = client_node_count + FLAGS.dsbpp_hotel_frontend_nodes + FLAGS.dsbpp_hotel_application_nodes + FLAGS.dsbpp_hotel_cachedb_nodes
        if min_required_node_count > total_node_count:
          raise errors.Benchmarks.PrepareException(f"Not enough nodes in cluster to assign application tiers to nodes. Have {total_node_count} nodes. Need at least {min_required_node_count} nodes.")
        unassigned_nodes = nodes
        for _ in range(client_node_count):
          _LabelClusterNode(benchmark_spec, unassigned_nodes[0], "client", "ok")
          unassigned_nodes.pop(0)
        for _ in range(FLAGS.dsbpp_hotel_frontend_nodes):
          _LabelClusterNode(benchmark_spec, unassigned_nodes[0], "frontend", "ok")
          unassigned_nodes.pop(0)
        for _ in range(FLAGS.dsbpp_hotel_application_nodes):
          _LabelClusterNode(benchmark_spec, unassigned_nodes[0], "application", "ok")
          unassigned_nodes.pop(0)
        for _ in range(FLAGS.dsbpp_hotel_cachedb_nodes):
          _LabelClusterNode(benchmark_spec, unassigned_nodes[0], "cachedb", "ok")
          unassigned_nodes.pop(0)
        if len(unassigned_nodes) > 0:
          logging.info(f"{len(unassigned_nodes)} nodes in the cluster will not be used.")

    def _TweakManifests():
      vm_util.RunThreaded(_TweakInstanceManifests,
                           list(range(FLAGS.dsbpp_hotel_namespace_count)))

    def _TweakInstanceManifests(inst_id):
      rootdir = os.path.join(vm_util.GetTempDir(), DSBPP_DIRECTORY, "hotelReservation", "k8s")
      deploydir = os.path.join(vm_util.GetTempDir(), 'dsbpp_deploy{}'.format(inst_id))
      vm_util.IssueCommand(['mkdir', '-p', deploydir])
      #if FLAGS.dsbpp_hotel_clients_in_cluster:
      #  vm_util.IssueCommand(['cp', data.ResourcePath(os.path.join('dsbpp_hotel', 'extras', 'wrk2.yaml')), rootdir])
      for filename in glob.glob(os.path.join(rootdir, "*.yaml")):
        if 'wrk-client' in filename and not FLAGS.dsbpp_hotel_clients_in_cluster:
          continue
        if 'persistentvolumeclaim' in filename:
          continue
        if 'jaeger-deployment' in filename and not FLAGS.dsbpp_hotel_deploy_jaeger:
          continue
        if 'cadvisor' in filename:
          continue
        if 'prometheus' in filename:
          continue
        # tweak manifests as needed
        new_filename = os.path.join(deploydir, os.path.basename(filename))
        with open(filename) as original_file:
          txt = original_file.read()
          manifest = yaml.safe_load_all(txt)
          with open(new_filename, 'w') as new_file:
            new_docs = []
            for doc in manifest:
              # update namespace
              doc['metadata']['namespace'] = _GetNameSpaceName(inst_id)
              if doc['kind'] == 'Service':
                # update service type
                if doc['metadata']['name'] in FRONTEND_SERVICES + TRACING_SERVICES_OUT:
                  doc['spec']['type'] = "LoadBalancer" if _IsManagedCluster() else "NodePort"
              if doc['kind'] == 'Deployment':
                # remove persistentvolumeclaim from mongodb deployments
                if "volumes" in doc["spec"]["template"]["spec"]:
                  if "persistentVolumeClaim" in doc["spec"]["template"]["spec"]["volumes"][0]:
                    del doc["spec"]["template"]["spec"]["volumes"][0]["persistentVolumeClaim"]
                    doc["spec"]["template"]["spec"]["volumes"][0]["emptyDir"] = {}
                # add CRI-RM pod affinity policies
                if FLAGS.dsbpp_hotel_enable_crirm:
                  r = ShadowOrderedDict()
                  affinity_policy = ""
                  anti_affinity_policy = ""

                  for container in doc['spec']['template']['spec']['containers']:
                    affinity_policy += AFFINITY_POLICY.format(container=container['name'],
                                                              namespace=_GetNameSpaceName(inst_id))
                    anti_affinity_policy += ANTI_AFFINITY_POLICY.format(container=container['name'],
                                                              namespace=_GetNameSpaceName(inst_id))

                  r[AFFINITY_KEY] = literal(affinity_policy)
                  #r[ANTI_AFFINITY_KEY] = literal(anti_affinity_policy)
                  doc['spec']['template']['metadata']['annotations'] = r

                # update image
                if doc['metadata']['name'] in APPLICATION_SERVICES + FRONTEND_SERVICES:
                  for container in doc['spec']['template']['spec']['containers']:
                    container['image'] = FLAGS.dsbpp_hotel_docker_image
                    # SetGCPercent env
                    if container.get('env', None):
                      container['env'].append({'name': 'GC', 'value': str(FLAGS.dsbpp_hotel_gcpercent)})
                    else:
                      container['env'] = [{'name': 'GC', 'value': str(FLAGS.dsbpp_hotel_gcpercent)}]
                    # MEMC_TIMEOUT env
                    if container.get('env', None):
                      container['env'].append({'name': 'MEMC_TIMEOUT', 'value': str(FLAGS.dsbpp_hotel_memctimeout)})
                    else:
                      container['env'] = [{'name': 'MEMC_TIMEOUT', 'value': str(FLAGS.dsbpp_hotel_memctimeout)}]
                    # Disable tracing
                    if not FLAGS.dsbpp_hotel_deploy_jaeger:
                      container['env'].append({'name': 'JAEGER_SAMPLE_RATIO', 'value': '0'})
                if FLAGS.dsbpp_hotel_image_cache:
                  # add cache path to image path
                  for container in doc['spec']['template']['spec']['containers']:
                    path = FLAGS.dsbpp_hotel_image_cache
                    container['image'] = path + container['image']
                # Update for hugepage
                if FLAGS.dsbpp_hotel_hugepage_size > 0 and doc['metadata']['name'] in APPLICATION_SERVICES + FRONTEND_SERVICES:
                  hp_size = "{}Mi".format(FLAGS.dsbpp_hotel_hugepage_size)
                  resources_def = {'limits':   {'memory': hp_size, 'hugepages-2Mi': hp_size},
                                   'requests': {'memory': hp_size},
                                  }
                  volumeMount_def = {'mountPath': '/hugepages',
                                     'name': 'hugepage',
                                     }
                  volume_def = {'name': 'hugepage',
                                'emptyDir': {'medium': 'HugePages'},
                               }
                  for container in doc['spec']['template']['spec']['containers']:
                    container['resources'] = resources_def
                    if 'volumeMounts' in container:
                        container['volumeMounts'].append(volumeMount_def)
                    else:
                        container['volumeMounts']=[volumeMount_def]
                  if 'volumes' in doc['spec']['template']['spec']:
                      doc['spec']['template']['spec']['volumes'].append(volume_def)
                  else:
                      doc['spec']['template']['spec']['volumes']=[volume_def]
                doc['spec']['template']['spec']['nodeSelector'] = {}
                if FLAGS.dsbpp_hotel_node_label:
                  # add label as a nodeSelector so that pods will only be deployed to nodes with this label
                  key, value = FLAGS.dsbpp_hotel_node_label.split('=')
                  doc['spec']['template']['spec']['nodeSelector'][key] = value
                if FLAGS.dsbpp_hotel_assign_nodes:
                  # seperate service types across nodes using nodeSelector
                  if doc['metadata']['name'] in APPLICATION_SERVICES:
                    doc['spec']['template']['spec']['nodeSelector']['application'] = 'ok'
                  elif doc['metadata']['name'] in FRONTEND_SERVICES:
                    doc['spec']['template']['spec']['nodeSelector']['frontend'] = 'ok'
                  elif doc['metadata']['name'] in CACHEDB_SERVICES + TRACING_SERVICES:
                    doc['spec']['template']['spec']['nodeSelector']['cachedb'] = 'ok'
                  elif doc['metadata']['name'] in CLIENT_SERVICES:
                    doc['spec']['template']['spec']['nodeSelector']['client'] = 'ok'
              new_docs.append(doc)
            yaml.safe_dump_all(new_docs, new_file)

         

    _TeardownApplication(benchmark_spec)
    _ConfigureNodeSelectors(benchmark_spec)
    _TweakManifests()
    _DeployApplication(benchmark_spec)


  def _GetDeathStarBenchArchive():
    target_dir = os.path.join(vm_util.GetTempDir(), DSBPP_DIRECTORY)
    target_file = os.path.join(target_dir, DSBPP_ARCHIVE)
    vm_util.IssueCommand(["mkdir", "-p", target_dir])
    if FLAGS.dsbpp_hotel_dsb_archive.startswith("http"):
      headers = {}
      downloadcmd = ["curl", "-sL", "-o", target_file]
      if FLAGS.dsbpp_hotel_github_token:
        downloadcmd.append("-H")
        downloadcmd.append("Authorization: token %s" % FLAGS.dsbpp_hotel_github_token)
        headers["Authorization"] = "token {}".format(FLAGS.dsbpp_hotel_github_token)
      downloadcmd.append(FLAGS.dsbpp_hotel_dsb_archive)
      req = requests.head(url=FLAGS.dsbpp_hotel_dsb_archive, headers=headers)
      if req.status_code == requests.codes.not_found:
        raise errors.Benchmarks.PrepareException('DSB source code package could not be fetched from {}'.format(
          FLAGS.dsbpp_hotel_dsb_archive))
      vm_util.IssueCommand(downloadcmd)
    else:
      vm_util.IssueCommand(["cp", FLAGS.dsbpp_hotel_dsb_archive, target_file])
    vm_util.IssueCommand(["tar", "-xf", target_file, "--strip-components=1", "-C", target_dir])

  def _SetCPUFreqGovernor():
    def _RunOnTarget(target):
        vm_util.SetCPUGovernor(target, 'performance')
    vm_util.RunThreaded(_RunOnTarget, _GetClients(benchmark_spec))
    vm_util.RunThreaded(_RunOnTarget, _GetWorkers(benchmark_spec))

  benchmark_spec.always_call_cleanup = True
  benchmark_spec.control_traces = True

  _SetCPUFreqGovernor()
  _GetDeathStarBenchArchive()
  _PrepareClients(_GetClients(benchmark_spec))
  _PrepareCluster(benchmark_spec)
  _SavePodConfig(benchmark_spec, "After Prepare")


def _GetAllPodsRestartCount(pods_json, inst_id):
  pods_restart_count = {}
  pods = json.loads(pods_json)
  for pod in pods["items"]:
    pod_name = 'inst{}-{}'.format(inst_id, pod["metadata"]["name"])
    containers_restart_count = {}
    for container in pod["status"]["containerStatuses"]:
      container_name = container["name"]
      restart_count = container["restartCount"]
      ready = container["ready"]
      containers_restart_count[container_name] = (restart_count, ready)
    if containers_restart_count:
      pods_restart_count[pod_name] = containers_restart_count
  return pods_restart_count


def _GetPodsEverReportedError(pods_restart_count_a, pods_restart_count_b):
  restarted_containers = []
  error_containers = []
  for pod, containers_restart_count in pods_restart_count_b.items():
    for container, (restart_count, ready) in containers_restart_count.items():
      if (pods_restart_count_a.get(pod) is None or
          restart_count != pods_restart_count_a[pod].get(container)[0]):
        restarted_containers.append((pod, container))
      if ready is False:
        error_containers.append((pod, container))
  restarted_containers = restarted_containers or None
  error_containers = error_containers or None
  return restarted_containers, error_containers


def _SaveContainerLogs(benchmark_spec,
                       file_name, pod,
                       container, inst_id,
                       last_run=False, tail=500):
  with open(file_name, 'w') as f:
    args = ['logs', pod, '-c', container, "--tail", str(tail)]
    if last_run:
      args.append('-p')
    message = _RunKubectl(benchmark_spec, args,
                          inst_id=inst_id,
                          suppress_failure=True)[0]
    f.write(message)


def _SaveAllContainerMessage(benchmark_spec, pods_json, folder, inst_id):
  pods = json.loads(pods_json)
  for pod in pods["items"]:
    pod_name = pod["metadata"]["name"]
    for container in pod["spec"]["containers"]:
      container_name = container["name"]
      file_name = f"{pod_name}_{container_name}.log"
      file_name = os.path.join(folder, file_name)
      _SaveContainerLogs(benchmark_spec, file_name, pod_name, container_name, inst_id)


def Run(benchmark_spec):
  results = []

  def _ParseWrkOutput(outputs, metadata):
    request_throughput = 0.0
    results_metadata = metadata.copy()
    percentile_pattern = re.compile(r"^\s*([0-9]{2,3}\.[0-9]{3}\%)\s*([0-9]+\.[0-9]*)(\w+)\s*$", flags=re.MULTILINE)
    counts_pattern = re.compile(r"^\s*([0-9]+)\s*requests\s*in\s([0-9]+\.[0-9]*)(\w),\s*([0-9]+\.[0-9]*)(\w+)\s*read$", flags=re.MULTILINE)
    errors_pattern = re.compile(r"^\s*Socket errors:\s*connect\s*([0-9]+),\s*read\s*([0-9]+),\s*write\s*([0-9]+),\s*timeout\s*([0-9]+)$", flags=re.MULTILINE)
    retcode_pattern = re.compile(r"^\s*Non-2xx or 3xx responses:\s*([0-9]+)$", flags=re.MULTILINE)
    requests_pattern = re.compile(r"^Requests/sec:\s*([0-9]*\.[0-9]*)$", flags=re.MULTILINE)
    for output in outputs:
      # percentiles
      def _MetadataPercentile(key, value, unit):
        """ record worst case for each percentile across all clients """
        v = time_to_ms(value, unit)
        try:
          if v > results_metadata[key]:
            results_metadata[key] = v
        except KeyError:
          results_metadata[key] = v
      for match in percentile_pattern.finditer(output):
        _MetadataPercentile(match.group(1), float(match.group(2)), match.group(3))

      # counts
      def _MetadataCount(key, value):
        try:
          results_metadata[key] += value
        except KeyError:
          results_metadata[key] = value
      match = counts_pattern.search(output)
      _MetadataCount("request_count", int(match.group(1)))
      results_metadata["time"] = match.group(2) + match.group(3)  # don't aggregate
      # socket errors
      match = errors_pattern.search(output)
      _MetadataCount("connect_error_count", int(match.group(1)) if match else 0)
      _MetadataCount("read_error_count", int(match.group(2)) if match else 0)
      _MetadataCount("write_error_count", int(match.group(3)) if match else 0)
      _MetadataCount("timeout_error_count", int(match.group(4)) if match else 0)
      # http errors
      match = retcode_pattern.search(output)
      _MetadataCount("http_error_count", int(match.group(1)) if match else 0)
      # throughput in requests (primary metric of interest)
      match = requests_pattern.search(output)
      request_throughput += float(match.group(1))
    return sample.Sample("Request Throughput", request_throughput, "requests/sec", results_metadata)

  def _AutoScale(benchmark_spec, workload):
    """start with the min rate and scale up the rate until exit condition is met"""
    samples = []
    throughputs = []
    rate = FLAGS.dsbpp_hotel_client_rate or FLAGS.dsbpp_hotel_client_rate_autoscale_min
    max_error_rate_exceeded_count = 2
    error_rate_exceeded_count = 0
    max_error_pod_failure_count = 10
    error_pod_failure_count = 0
    max_error_redeploy_count = 10
    error_redeploy_count = 0
    iteration_sample = None
    termination = None

    while True:
      error_rate_exceeded = False

      def _RunOnClient(client, command):
        return client.RobustRemoteCommand(command, should_log=True)[0]

      def _RunOnCluster(benchmark_spec, pod, command, inst_id):
        return _RunKubectl(benchmark_spec,
                           ['exec', pod, "--"] + command.split(' '),
                           inst_id=inst_id,
                           timeout=FLAGS.dsbpp_hotel_client_duration + 10,
                           force_info_log=True)[0]

      command_fmt = "{} -D exp -T {}s -t {} -c {} -d {} -L -s {} {} -R {}".format(
          "{}",
          FLAGS.dsbpp_hotel_client_timeout,
          _GetClientThreadCount(benchmark_spec),
          _GetClientConnectionCount(benchmark_spec),
          FLAGS.dsbpp_hotel_client_duration,
          "{}",
          "{}",
          rate)

      pods_restart_count_start = {}
      for inst_id in range(FLAGS.dsbpp_hotel_namespace_count):
        pods_json = _RunKubectl(benchmark_spec, ['get', 'pods', '-o', 'json'],
                                inst_id=inst_id)[0]
        restart_count = _GetAllPodsRestartCount(pods_json, inst_id)
        pods_restart_count_start.update(restart_count)


      if FLAGS.trace_allow_benchmark_control and IsAnyTraceEnabled():
        events.start_trace.send(stages.RUN, benchmark_spec=benchmark_spec)

      if FLAGS.dsbpp_hotel_clients_in_cluster:
        url = "http://{}:{}".format(_GetSvcIpAddress(benchmark_spec, "frontend", inst_id), WEB_FRONTEND_PORT)
        wrk2 = posixpath.join("socialNetwork", "wrk2", "wrk")
        script = posixpath.join("hotelReservation", "wrk2_lua_scripts", workload["script"])
        command = command_fmt.format(wrk2, script, url)
        client_outputs = vm_util.RunThreaded(_RunOnCluster,
                                             [((benchmark_spec, pod, command, inst_id), {}) for pod in _GetWrk2Pods(benchmark_spec, inst_id)])
      else:
        commands = []
        wrk2 = "./wrk"
        script = posixpath.join(WRK2_SCRIPTS_DIR, workload["script"])
        for inst_id in range(FLAGS.dsbpp_hotel_namespace_count):
          url = "http://{}:{}".format(_GetWebIpAddress(benchmark_spec, "frontend", inst_id),
                                      _GetWebPort(benchmark_spec, "frontend", inst_id))
          command = "cd {} && ".format(WRK2_DIR) + command_fmt.format(wrk2, script, url)
          commands.append(command)

      client_outputs = vm_util.RunThreaded(
        _RunOnClient,
        [((client_vm, command), {})
         for command in commands
         for client_vm in _GetClients(benchmark_spec)
         for _ in range(FLAGS.dsbpp_hotel_client_instances_per_vm)])
        #[((client_vm, command), {}) for client_vm in _GetClients(benchmark_spec) for _ in range(FLAGS.dsbpp_hotel_client_instances_per_vm)])

      if FLAGS.trace_allow_benchmark_control and IsAnyTraceEnabled():
          events.stop_trace.send(stages.RUN, benchmark_spec=benchmark_spec, prefix=rate)

      pods_restart_count_end = {}

      # check if any pod or container is in failure state after running
      for inst_id in range(FLAGS.dsbpp_hotel_namespace_count):
        pods_json = _RunKubectl(benchmark_spec, ['get', 'pods', '-o', 'json'],
                                inst_id=inst_id)[0]
        restart_count = _GetAllPodsRestartCount(pods_json, inst_id)
        pods_restart_count_end.update(restart_count)

      restarted_containers, error_containers = _GetPodsEverReportedError(pods_restart_count_start, pods_restart_count_end)

      if restarted_containers or error_containers:
        tempdir = vm_util.GetTempDir()
        log_root_dir = "error_container_logs"
        log_rate_dir = f"rate{rate}-retry{error_redeploy_count}"
        folder = os.path.join(tempdir, log_root_dir, log_rate_dir)
        os.makedirs(folder, exist_ok=True)

        warning_containers = []
        if restarted_containers is not None:
          warning_containers += [(i, True) for i in restarted_containers]
        if error_containers is not None:
          warning_containers += [(i, False) for i in error_containers]

        for (pod, container), last_run in warning_containers:
          lastrun = 'lastrun' if last_run else 'currun'
          file_name = os.path.join(folder, f"{pod}-{container}.{lastrun}.log")
          logging.info(f"Save restarted container logging message: {file_name}")
          match = re.match(r'inst(\d+)-(.*)', pod)
          inst_id = int(match.group(1))
          pod_name = match.group(2)
          _SaveContainerLogs(benchmark_spec, file_name, pod_name, container,
                             inst_id, last_run=last_run)

        error_redeploy_count += 1
        if error_redeploy_count >= max_error_redeploy_count:
          logging.info("Max retries on redeploying containers. Terminating")
          if iteration_sample:
            iteration_sample.metadata["Termination"] = "Max retries on containers recovery from error."
          termination = "Max retries on containers recovery from error."
          break
        logging.info("pod error: redeploy and test again at rate {}. Retry {} ".format(rate, error_redeploy_count))
        _TeardownApplication(benchmark_spec)
        _DeployApplication(benchmark_spec)
        continue
      error_redeploy_count = 0

      # record the state of the application
      _SavePodConfig(benchmark_spec,
                     f"After collection with a client rate of {rate}")
      # parse the client output
      iteration_sample = _ParseWrkOutput(client_outputs,
                                         {'workload': workload["name"],
                                          'rate': rate,
                                          'namespace_count': FLAGS.dsbpp_hotel_namespace_count,
                                          'rate_total': rate * len(_GetClients(benchmark_spec)) * FLAGS.dsbpp_hotel_client_instances_per_vm * FLAGS.dsbpp_hotel_namespace_count,
                                          'duration': FLAGS.dsbpp_hotel_client_duration,
                                          'threads': _GetClientThreadCount(benchmark_spec),
                                          'connections': _GetClientConnectionCount(benchmark_spec),
                                          'connections_total': _GetTotalClientConnectionCount(benchmark_spec),
                                          }
                                         )
      # did the run exceed the allowable number of errors from the client?
      error_rate_exceeded = \
          iteration_sample.metadata["http_error_count"] + \
          iteration_sample.metadata["connect_error_count"] + \
          iteration_sample.metadata["read_error_count"] + \
          iteration_sample.metadata["write_error_count"] + \
          iteration_sample.metadata["connect_error_count"] \
          > iteration_sample.metadata["request_count"] * FLAGS.dsbpp_hotel_error_limit
      # was the SLA exceeded?
      sla_exceeded = iteration_sample.metadata["99.000%"] > FLAGS.dsbpp_hotel_p99_sla

      if error_rate_exceeded:
        error_rate_exceeded_count += 1
        logging.info("Error rate exceeded.")
        if error_rate_exceeded_count < max_error_rate_exceeded_count:
          logging.info("Try again at same rate ({})".format(rate))
      #elif not pod_ok or after_restart_count > before_restart_count:
      #  error_pod_failure_count += 1
      #  logging.info("Some({}) workload pods in error status or restart during running.".format(str(after_restart_count - before_restart_count)))
      #  if error_pod_failure_count < max_error_pod_failure_count:
      #    logging.info("Try again at same rate ({})".format(rate))
      else:
        # (NOTE) If no error happens, trigger the emon post process every run and rename the edp tarball file with
        # the tps number.
        if FLAGS.emon and FLAGS.trace_allow_benchmark_control and IsAnyTraceEnabled():
          worker_count = len(benchmark_spec.vm_groups["worker"])
          tps = iteration_sample.value / worker_count
          logging.info("Trigger the emon post processing in dsbpp hotel benchmark with tps {0}...".format(tps))
          events.trace_post_process.send(
            stages.RUN, benchmark_spec=benchmark_spec,
            sample=iteration_sample,
            prefix=rate,
            worker_count=worker_count)
        error_rate_exceeded_count = 0
        #error_pod_failure_count = 0
        error_redeploy_count = 0
        samples.append(iteration_sample)
        throughputs.append(iteration_sample.value)
        rate += FLAGS.dsbpp_hotelpp_client_rate_autoscale_increment

      # are we done?  check all conditions for exit.
      if autoscale_util.MeetsExitCriteria(throughputs,times=FLAGS.dsbpp_hotel_maxtimes):
        logging.info("Performance degrading. Terminating.")
        termination = "Performance degrading."
        break
      if FLAGS.dsbpp_hotel_p99_sla_only and sla_exceeded:
        logging.info("p99 SLA exceeded. Terminating.")
        termination = "p99 SLA exceeded."
        break
      #if error_pod_failure_count == max_error_pod_failure_count:
      #  logging.info("Max retries on pod errors met. Terminating.")
      #  termination = "Max retries on pod errors met."
      #  break
      if error_rate_exceeded_count == max_error_rate_exceeded_count:
        logging.info("Max retries on wrk errors met. Terminating.")
        termination = "Max retries on wrk errors met."
        break
      if FLAGS.dsbpp_hotel_client_rate_autoscale_max and rate > FLAGS.dsbpp_hotel_client_rate_autoscale_max:
        logging.info("Met user-specified maximum rate: {}. Terminating".format(FLAGS.dsbpp_hotel_client_rate_autoscale_max))
        termination = "Met user-specified maximum rate."
        break
      if FLAGS.dsbpp_hotel_client_rate:
        logging.info("User specified a single rate. Terminating.")
        termination = "User specified a single rate."
        break
      # reset the application in preparation for next iteration
      if not FLAGS.dsbpp_hotel_skip_teardown:
        _TeardownApplication(benchmark_spec)
        _DeployApplication(benchmark_spec)

    # find the max and max_sla throughputs
    throughput_samples = [s for s in samples if s.metric == "Request Throughput"]
    if throughput_samples:
      max_throughput = 0
      max_sla_throughput = 0
      max_sample = None
      max_sla_sample = None
      for ts in throughput_samples:
        if ts.value > max_throughput:
          max_throughput = ts.value
          max_sample = ts
        if ts.metadata["99.000%"] <= FLAGS.dsbpp_hotel_p99_sla and ts.value > max_sla_throughput:
          max_sla_throughput = ts.value
          max_sla_sample = ts
      if max_sample and not FLAGS.dsbpp_hotel_p99_sla_only:
        max_metadata = max_sample.metadata.copy()
        max_metadata['Termination'] = termination
        samples.append(sample.Sample("Maximum Throughput",
                                     max_sample.value, max_sample.unit, max_metadata))
      if max_sla_sample:
        max_sla_metadata = max_sla_sample.metadata.copy()
        max_sla_metadata['Termination'] = termination
        max_sla_metadata["p99 SLA (ms)"] = FLAGS.dsbpp_hotel_p99_sla
        samples.append(sample.Sample("Maximum Throughput with SLA",
                                     max_sla_sample.value, max_sla_sample.unit, max_sla_metadata))
    else:
      if not FLAGS.dsbpp_hotel_p99_sla_only:
        samples.append(sample.Sample("Maximum Throughput",
                                     0, "requests/sec", {'workload': workload["name"]}))
      samples.append(sample.Sample("Maximum Throughput with SLA",
                                   0, "requests/sec", {'workload': workload["name"], 'p99 SLA (ms)': FLAGS.dsbpp_hotel_p99_sla}))
    return samples

  # user may want to change replica counts between repeated 'run' requests at a specific
  # run rate, e.g. when using Adapative Optimization to drive the workload
  if len(FLAGS.run_stage) == 1:  # only the "run" run_stage was requested
    if FLAGS.dsbpp_hotel_client_rate is not None:  # a specific rate was specified (not auto-scaling)
      if not FLAGS.dsbpp_hotel_skip_teardown:
        _TeardownApplication(benchmark_spec)
      _DeployApplication(benchmark_spec)

  workloads = []
  if "mixed-workload_type_1" in FLAGS.dsbpp_hotel_workloads:
    workloads.append({"name": "mixed-workload_type_1",
                      "script": "mixed-workload_type_1.lua"})
  for workload in workloads:
    results.extend(_AutoScale(benchmark_spec, workload))

  return results


def Cleanup(benchmark_spec):
  _TeardownApplication(benchmark_spec)
