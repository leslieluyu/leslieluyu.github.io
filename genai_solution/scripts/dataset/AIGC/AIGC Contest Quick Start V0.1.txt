Intel AIGC Contest (2025)开发者指南 TODO: put the guideline to AIGC contest website in HTML formatContents一、开发环境及准备	21.	硬件配置 – AI PC	22.	软件配置以及环境准备 – AI PC	23.	硬件配置 – Edge	34.	软件环境以及环境准备 – Edge	3二、Intel AI 推理框架在线资源	31.	Intel openVINO [AI PC ✓, Edge✓], [Windows✓, Linux✓]	32.	Pytorch (IPEX-LLM)  [AI PC ✓, Edge✓], [Windows✓, Linux✓]	53.	Pytorch (IPEX)  [AI PC ✓, Edge✓], [Windows✓, Linux✓]	64.	Intel AIPC open gateway (AOG)  [AI PC ✓, Edge×], [Windows✓, Linux✓]	65.	Coze-AI PC   [AI PC ✓, Edge×], [Windows✓, Linux×]	7三、AIGC Contest Developer编程实战举例	71.	Intel openVINO编程步骤举例	72.	Pytorch(IPEX-LLM) 编程步骤举例	83.	Pytorch(IPEX) 编程步骤举例	84.	Intel AOG 编程步骤举例	85.	Coze-AI PC 编程步骤举例	9一、开发环境及准备1. 硬件配置 – AI PC• Intel® Core™ Ultra processors (Series 1, MTL) o Lenovo Thinkbook (Ultra 7 155H, 32GB DDR, 1TB SSD)o HP/Lenovo?• Intel® Core™ Ultra processors (Series 2, LNL) o Asus  Vivobook (Ultra 7 258V, 32GB DDR, 1TB SSD)o HP/Lenovo?2. 软件配置以及环境准备 – AI PC• Windows 11 24H2o Intel driver version: iGPU (>32.0.101.5972), NPU()o openVINO version: o Pytorch (IPEX-LLM):o Pytorch (IPEX):o Coze-AIPC: o AOG:• Ubuntu Linux 24.10o Intel driver version: iGPU, NPUo openVINO version:o Pytorch (IPEX-LLM)o Pytorch (IPEX)• 安装方式：o Intel GPU on Windows: https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/install_windows_gpu.mdo Intel GPU on Linux: https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/install_linux_gpu.mdo Intel NPU: https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/npu_quickstart.mdo Docker: https://github.com/intel-analytics/ipex-llm/tree/main/docs/mddocs/DockerGuideso OpenVINO runtime：pip install openvino o OpenVINO with GenAI： pip install openvino-genai3. 硬件配置 – Edge• Intel(R) Xeon(R) w7-3455, 4x A770-16GB, 512GB DDR• HP/Lenovo?4. 软件环境以及环境准备 – Edge• Ubuntu Linux 22.04 LTS, K6.5• vLLM-Serving Docker with IPEX-LLM: intelanalytics/ipex-llm-serving-xpu:2.2.0-b11• 安装步骤:• 二、Intel AI 推理框架在线资源1. Intel openVINO	[AI PC ✓, Edge✓], [Windows✓, Linux✓] OpenVINO™ 是一个深度学习模型的部署工具，可实现针对特定模型结构的优化与推理任务提升，目前 OpenVINO™ 已适配x86，ARM在内的异构硬件终端，支持CNN，Transformer等多种网络结构，提供C/C++/Python等多类API接口语言，不管是传统的机器视觉、NLP模型，或是最新的LLM、多模态模型，都可以利用 OpenVINO™工具包中提供的丰富接口进行部署。• 在线文档:o Documents：https://docs.openvino.ai/o Github：https://github.com/openvinotoolkito Gitee：https://gitee.com/openvinotoolkit-prco Notebooks示例：https://openvinotoolkit.github.io/openvino_notebooks/o Video: https://space.bilibili.com/38566875o WeChat: • YOLO视觉模型示例o https://github.com/openvinotoolkit/openvino_notebooks/tree/latest/notebooks/yolov11-optimizationo https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/notebooks/yolov10-optimization/yolov10-optimization.ipynb• SAM分割示例o https://github.com/openvinotoolkit/openvino_notebooks/tree/latest/notebooks/sam2-image-segmentation• 文生图示例：o 性能最优：• https://github.com/openvinotoolkit/openvino_notebooks/tree/latest/notebooks/latent-consistency-models-image-generation• https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/notebooks/sdxl-turbo/sdxl-turbo.ipynbo 效果最佳：• https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/notebooks/flux.1-image-generation/flux.1-image-generation.ipynb• https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/notebooks/stable-diffusion-v3/stable-diffusion-v3.ipynb• Whisper语音识别示例o https://github.com/openvinotoolkit/openvino_notebooks/tree/latest/notebooks/distil-whisper-asR• Chatbot示例（支持多种模型）o https://github.com/openvinotoolkit/openvino_notebooks/tree/latest/notebooks/llm-chatbot• RAG示例o https://github.com/openvinotoolkit/openvino_notebooks/tree/latest/notebooks/llm-rag-llamaindexo https://github.com/openvinotoolkit/openvino_notebooks/tree/latest/notebooks/llm-rag-langchain• 多模态示例o https://github.com/openvinotoolkit/openvino_notebooks/tree/latest/notebooks/qwen2-vlo https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/notebooks/qwen2-audio/qwen2-audio.ipynb• Agent智能体示例o https://github.com/openvinotoolkit/openvino_notebooks/tree/latest/notebooks/llm-agent-functioncallo https://github.com/openvinotoolkit/openvino_notebooks/tree/latest/notebooks/llm-agent-react2. Pytorch (IPEX-LLM) 	[AI PC ✓, Edge✓], [Windows✓, Linux✓]IPEX-LLM（Intel® LLM Library for PyTorch*）是一个轻量级大模型加速库。它可以高效且无缝地将大模型运行于 Intel XPU 上（如搭载集成显卡和NPU的个人电脑，独立显卡像是Arc、BMG、Flex、PVC，以及CPU），以获得先进的大模型算法优化， XPU加速以及低比特（FP8/FP6/FP4/INT4）支持。70+ 模型已经在 IPEX-LLM 上得到优化和验证（如Llama3.2，Qwen2.5，MiniCPM，GLM-Edge，Qwen2-VL，MiniCPM-V 等）。• 官方文档：o GitHub Repo: https://github.com/intel-analytics/ipex-llmo Documents: https://github.com/intel-analytics/ipex-llm/tree/main/docs/mddocso Examples: https://github.com/intel-analytics/ipex-llm/tree/main/python/llm/exampleo QuickStarts: https://github.com/intel-analytics/ipex-llm/tree/main/docs/mddocs/Quickstarto Tutorials: https://github.com/intel-analytics/ipex-llm-tutorialo Video: https://www.igbacenter.com/course/detail_93.html• GPU模型加速示例：o 大型语言模型：https://github.com/intel-analytics/ipex-llm/tree/main/python/llm/example/GPU/HuggingFace/LLMo 多模态大模型：https://github.com/intel-analytics/ipex-llm/tree/main/python/llm/example/GPU/HuggingFace/Multimodalo Stable Diffusion：https://github.com/intel-analytics/ipex-llm/tree/main/python/llm/example/GPU/HuggingFace/Multimodal/StableDiffusion• GPU大模型应用示例：o Llama.cpp：https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/llama_cpp_quickstart.mdo Ollama：https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_quickstart.mdo Local RAG：o https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/graphrag_quickstart.mdo https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/ragflow_quickstart.mdo https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/chatchat_quickstart.mdo LangChain：o https://github.com/intel-analytics/ipex-llm/tree/main/python/llm/example/GPU/LangChaino LlamaIndex：o https://github.com/intel-analytics/ipex-llm/tree/main/python/llm/example/GPU/LlamaIndex• GPU大模型服务化示例：o vLLM：• https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/DockerGuides/vllm_docker_quickstart.md• https://github.com/intel-analytics/ipex-llm/tree/main/python/llm/example/GPU/vLLM-Servingo FastChat• https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/fastchat_quickstart.md• NPU大模型加速示例：o Python API：• 大语言模型：https://github.com/intel-analytics/ipex-llm/tree/main/python/llm/example/NPU/HF-Transformers-AutoModels/LLM• 多模态大模型：https://github.com/intel-analytics/ipex-llm/tree/main/python/llm/example/NPU/HF-Transformers-AutoModels/Multimodalo C++ API：• https://github.com/intel-analytics/ipex-llm/tree/main/python/llm/example/NPU/HF-Transformers-AutoModels/LLM/CPP_Examples3. Pytorch (IPEX) 	[AI PC ✓, Edge✓], [Windows✓, Linux✓]4. Intel AIPC open gateway (AOG) 	[AI PC ✓, Edge×], [Windows✓, Linux✓]• AOG把AI应用与它所依赖的AI服务进行解耦。AI应用只需通过 RESTful API使用相应的AI服务，而具体的AI服务的提供，运行和管理则交由系统里运行的AOG服务。这些AI服务可能是基于高度优化过的推理引擎和模型的本地服务，也可能是远程的云端服务。AOG会适配它们的API，并在本地服务提供者和云端服务提供者之间按需切换。• 官方网址：http://www.github.com/intel/aog5. Coze-AI PC	 	[AI PC ✓, Edge×], [Windows✓, Linux×]Coze-AIPC是英特尔联合扣子(https://www.coze.cn/)发布的行业首个支持端云协同的智能体开发平台。即将上线运营的Coze-AIPC平台引入了一款全新的扣子PC端App，它利用扣子平台最新发布的“端插件”机制，将英特尔AI PC的强大端侧能力无缝集成到扣子大模型或者工作流中，共同构造“端云协同智能体新体验”。扣子PC端App将预制一套针对英特尔AI PC充分调优的端侧能力集。首期发布版本将包括PC操作，本地向量知识库，未来还将逐步引入本地推理，本地生成等能力。• 官方文档：o 扣子主页：https://www.coze.cn/o 扣子智能体搭建指南：https://www.coze.cn/docs/guides/welcomeo 扣子学习指南：https://www.coze.cn/docs/guides/learning_resourceso 扣子AIPC Local App：待上线• 端云协同智能体场景示例：o 个人生活助手：Coze-AIPC个人生活助手.mp4o 实时视频理解，支持影视，体育等场景智能问答o 识别关键画面，提炼剧情重点和精彩瞬间o 智能体灵活操控本地PC设备，实现智能场景高度自动化o 智能工作助手：Coze-AIPC智能工作助手.mp4o 隐私文档本地处理，端云融合RAG智能生成可视化报表分析o 专业文档写作，表单填写，针对本地个人隐私数据库，输出契合个人和行业特点的优质内容三、AIGC Contest Developer编程实战举例1. Intel openVINO编程步骤举例o For Example: LNL Windows/Linux openVINO编程步骤o 2. Pytorch(IPEX-LLM) 编程步骤举例o For Example: LNL Windows/Linux iGPU进行LLM推理• https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Overview/install_gpu.md3. Pytorch(IPEX) 编程步骤举例o For Examples: Classification & Detection4. Intel AOG 编程步骤举例• 手动部署优化版本地chat服务 （注：AOG未来版本会自动化安装这一步）o 下载Intel® IPEX-LLM优化版本的ollama.zip 试用包o 解压并启动ollamao 运行ollama run glm4  （可以替换成自己希望使用的模型，具体可参考ollama文档）• 下载AOG并进行配置 – 具体可参见自动的opac.config.example，并参照相应文档o 配置本地chat 服务 – 例如上一步安装的本地ollama o 配置云端服务，加入预先获取的相应的云端AI服务的API Key/Token• 启动AOGo 运行 aog -config <path_to_your_config> o 可以用curl 来测试AOG及对应的chat服务是否安装和配置成功。示例如下，其中的model, stream，hybrid_policy可视情况调整。curl http://localhost:16688/aog/v0.2/services/chat -X POST -H "Content-Type: application/json" -d "{\"model\":\"glm4\",\"messages\":[{\"role\":\"user\",\"content\":\"why is the sky blue?\"}],\"options\":{\"seed\":12345,\"temperature\":0},\"hybrid_policy\":\"always_local\", \"stream\":false}"• 在自己的程序里面调用本地AOG的chat RESTful API• 也可使用ollama或者openAI等风格的chat API调用5. Coze-AI PC 编程步骤举例o 扣子智能体端云协同实现方式o 端插件注册进基础大模型         o 端插件嵌入扣子工作流