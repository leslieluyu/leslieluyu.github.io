apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "207"
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"name":"llm-deploy","namespace":"default"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"llm-deploy"}},"template":{"metadata":{"annotations":{"sidecar.istio.io/rewriteAppHTTPProbers":"true"},"labels":{"app":"llm-deploy"}},"spec":{"containers":[{"command":["python3","llm_inference_api.py"],"env":[{"name":"OMP_NUM_THREADS","value":"16"},{"name":"MODEL_NAME","value":"chatglm2-6b"},{"name":"MODEL_PATH","value":"/models/chatglm2-6b"},{"name":"MODEL_DTYPE","value":"fp16"},{"name":"SVC_PORT","value":"8000"},{"name":"METRIC_PORT","value":"8090"}],"image":"ccr-registry.caas.intel.com/cnbench/llm-inference-api:chatglm2.v1-metrics-port","name":"llm-deploy-demo","ports":[{"containerPort":8000},{"containerPort":8090}],"resources":{"limits":{"cpu":"16000m","memory":"26Gi"},"requests":{"cpu":"16000m","memory":"26Gi"}},"volumeMounts":[{"mountPath":"/models","name":"model-volume"}]}],"nodeSelector":{"llmdemo":"true"},"serviceAccountName":"default","volumes":[{"hostPath":{"path":"/mnt/model","type":"Directory"},"name":"model-volume"}]}}}}
  creationTimestamp: "2023-08-31T09:15:49Z"
  generation: 633
  name: llm-deploy
  namespace: default
  resourceVersion: "4261408"
  uid: 58b4efea-f368-4fc8-9141-1bf10c84f235
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: llm-deploy
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      annotations:
        sidecar.istio.io/rewriteAppHTTPProbers: "true"
      creationTimestamp: null
      labels:
        app: llm-deploy
    spec:
      containers:
      - command:
        - /usr/bin/numactl
        - -C
        - 0-55
        - python3
        - llm_inference_api.py
        env:
        - name: OMP_NUM_THREADS
          value: "56"
        - name: MODEL_NAME
          value: chatglm2-6b
        - name: MODEL_PATH
          value: /models/chatglm2-6b
        - name: MODEL_DTYPE
          value: fp16
        - name: SVC_PORT
          value: "8000"
        - name: METRIC_PORT
          value: "8090"
        image: ccr-registry.caas.intel.com/cnbench/llm-inference-api:chatglm2.v1-metrics-port
        imagePullPolicy: IfNotPresent
        name: llm-deploy-demo
        ports:
        - containerPort: 8000
          protocol: TCP
        - containerPort: 8090
          protocol: TCP
        resources:
          limits:
            cpu: "56"
            memory: 26Gi
          requests:
            cpu: "56"
            memory: 26Gi
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /models
          name: model-volume
      dnsPolicy: ClusterFirst
      nodeSelector:
        llmdemo: "true"
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      serviceAccount: default
      serviceAccountName: default
      terminationGracePeriodSeconds: 30
      volumes:
      - hostPath:
          path: /mnt/model
          type: Directory
        name: model-volume
status:
  availableReplicas: 1
  conditions:
  - lastTransitionTime: "2023-08-31T09:15:49Z"
    lastUpdateTime: "2023-09-21T08:49:34Z"
    message: ReplicaSet "llm-deploy-78f847676b" has successfully progressed.
    reason: NewReplicaSetAvailable
    status: "True"
    type: Progressing
  - lastTransitionTime: "2023-09-21T08:49:43Z"
    lastUpdateTime: "2023-09-21T08:49:43Z"
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: "True"
    type: Available
  observedGeneration: 633
  readyReplicas: 1
  replicas: 1
  updatedReplicas: 1
