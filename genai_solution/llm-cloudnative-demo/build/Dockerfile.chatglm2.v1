ARG UBUNTU_VER=22.04
FROM ubuntu:${UBUNTU_VER} as demo

# See http://bugs.python.org/issue19846
ENV LANG C.UTF-8
#ENV HTTP_PROXY=http://proxy-dmz.intel.com:911
#ENV HTTPS_PROXY=http://proxy-dmz.intel.com:912
ENV NO_PROXY=localhost,127.0.0.1

# ARG BRANCH=main
# ARG REPO=https://github.com/intel-sandbox/cloud.performance.generative.ai.workload.git

# Install system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends --fix-missing \
    python3.10 \
    python3.10-venv \
    python3-pip \
    python3-dev \
    python3-distutils \
    autoconf \
    build-essential \
    git \
    libgl1-mesa-glx \
    libglib2.0-0 \
    numactl \
    time \
    wget \
    vim \
    prometheus_client
RUN ln -s /usr/bin/python3.10 /usr/bin/python
# RUN ln -sf $(which python3) /usr/bin/python

RUN python -m pip --no-cache-dir install --upgrade pip
RUN python -m pip install --no-cache-dir setuptools

# Download the code
# RUN git clone --single-branch --branch=${BRANCH} ${REPO} /llm-workload
WORKDIR /llm-workload

COPY llm_inference_api.py llm_inference_api.py
COPY requirements-chatglm2-transformers.txt requirements.txt
RUN pip install -r requirements.txt --no-cache-dir

# KMP
ENV KMP_BLOCKTIME=1
ENV KMP_SETTINGS=1
ENV KMP_AFFINITY=granularity=fine,compact,1,0

# OMP
ENV OMP_NUM_THREADS=40

CMD exec numactl -C 0-39 -m 0 python llm_inference_api.py
