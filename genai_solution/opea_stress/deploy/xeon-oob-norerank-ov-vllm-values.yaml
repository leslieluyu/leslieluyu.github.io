# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

# Default values for chatqna.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1
LOGFLAG: "True"
image:
  repository: opea/chatqna-without-rerank
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: "${IMGVER}"

port: 8888
service:
  type: NodePort
  port: 8888
  nodePort: 30888

nginx:
  service:
    type: NodePort

securityContext:
  readOnlyRootFilesystem: true
  allowPrivilegeEscalation: false
  runAsNonRoot: true
  runAsUser: 1000
  capabilities:
    drop:
    - ALL
  seccompProfile:
    type: RuntimeDefault

nodeSelector: {}

tolerations: []

affinity: {}

# This is just to avoid Helm errors when HPA is NOT used
# (use hpa-values.yaml files to actually enable HPA).
autoscaling:
  enabled: false


vllm:
  accelDevice: "gaudi"
  image:
    repository: opea/vllm
    tag: "openvino"
  #extraCmdArgs: ["/bin/bash","-c","export VLLM_GRAPH_RESERVED_MEM=0.1 && export VLLM_GRAPH_PROMPT_RATIO=0.8 && export PT_HPU_ENABLE_LAZY_COLLECTIVES=true && export VLLM_SKIP_WARMUP=true && export VLLM_PROMPT_USE_FUSEDSDPA=1 && python3 -m vllm.entrypoints.openai.api_server --model Qwen/Qwen2-72B-Instruct --trust-remote-code --tensor-parallel-size 4 --dtype auto  --host 0.0.0.0 --port 2080 --download-dir /data --block-size 128 --max-num-seqs 128 --max-seq_len-to-capture 8192 --num-lookahead-slots 1 --use-v2-block-manager --enable-delayed-sampling --gpu-memory-utilization 0.9"]
  #extraCmdArgs: ["/bin/bash","-c","export VLLM_OPENVINO_KVCACHE_SPACE=100 && export VLLM_OPENVINO_CPU_KV_CACHE_PRECISION=u8 && export VLLM_OPENVINO_ENABLE_QUANTIZED_WEIGHTS=ON && python3 vllm/benchmarks/benchmark_throughput.py --model meta-llama/Llama-2-7b-chat-hf --dataset vllm/benchmarks/ShareGPT_V3_unfiltered_cleaned_split.json --enable-chunked-prefill --max-num-batched-tokens 256
  extraCmdArgs: ["/bin/bash","-c","export VLLM_CPU_KVCACHE_SPACE=50 && python3 -m vllm.entrypoints.openai.api_server --model Intel/neural-chat-7b-v3-3  --download-dir /data --host 0.0.0.0 --port 2080"]



# If you would like to switch to traditional UI image
# Uncomment the following lines
chatqna-ui:
  image:
    repository: "opea/chatqna-ui"
    tag: "${IMGVER}"
  containerPort: "5173"
tei:
  image:
    repository: ghcr.io/huggingface/text-embeddings-inference
    tag: cpu-1.5
  resources:
    limits:
      habana.ai/gaudi: 0
  evenly_distributed: True
data-prep:
  image:
    repository: opea/dataprep-redis
    tag: "${IMGVER}"
  port: 6010
  evenly_distributed: True
  livenessProbe:
    httpGet:
      path: v1/health_check
      port: data-prep
    initialDelaySeconds: 5
    periodSeconds: 5
    failureThreshold: 2400
  LOGFLAG: "True"

retriever-usvc:
  image:
    repository: opea/retriever-redis
    tag: "${IMGVER}"
  port: 7000
  evenly_distributed: True
  LOGFLAG: "True"
embedding-usvc:
  image:
    repository: opea/embedding-tei
    tag: "${IMGVER}"
  evenly_distributed: True
  LOGFLAG: "True"
llm-uservice:
  image:
    repository: opea/llm-vllm
    tag: "${IMGVER}"
  LLM_MODEL_ID: Intel/neural-chat-7b-v3-3
  evenly_distributed: True
  LOGFLAG: "True"


global:
  http_proxy: ""
  https_proxy: ""
  no_proxy: ""
  HUGGINGFACEHUB_API_TOKEN: "insert-your-huggingface-token-here"
  # set modelUseHostPath or modelUsePVC to use model cache.
  modelUseHostPath: ${MODELDIR}
  # modelUseHostPath: /mnt/opea-models
  # modelUsePVC: model-volume

  # Install Prometheus serviceMonitors for service components
  monitoring: false

  # Prometheus Helm install release name needed for serviceMonitors
  prometheusRelease: prometheus-stack


